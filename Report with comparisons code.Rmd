---
title: "Comparison of mBIC2 information criterion with methods provided in L0Learn library"
author: "Aleksander Milach"
date: "22/02/2021"
geometry: "left=3cm,right=3cm,top=1cm,bottom=1cm"
output: pdf_document
---

```{r setup, include=FALSE, fig.height=5}
knitr::opts_chunk$set(echo = FALSE)
```

We provide results of the simulation study comparing mBIC2 information criterion with functions from \textit{L0Learn} library. In all simulations we consider a multiple regression model with $n = p = 500$, the error term $\epsilon$ is iid normally distributed $N(0,I)$, the number $k$ of nonzero coefficients in the vector $\beta$ takes values $k\in \{10, 20, 40, 60, 80, 100\}$. We distinguish two levels of signal strenght. We associate weak signal with $$\beta_1 = \beta_2 = ... = \beta_k = 1.3 \sqrt{2 \log p} $$ and strong signal with $$\beta_1 = \beta_2 = ... = \beta_k = 2 \sqrt{2 \log p}. $$ The rows of design matrix $X$ are iid random vectors from multivariate normal distribution $N \left(0, \cfrac{1}{n} \Sigma \right)$. We consider two scenarios, one with independent regressors, where $\Sigma = I$, and one with correlated regressors, where $\Sigma_{i,j} = 1$, when $i=j$ and $\Sigma_{i,j} = 0.5$, when $i \neq j$.

For each simulation we calculate values of four statistics: false discovery rate (\textbf{FDR}) defined as avarage of #FP/max(1, #TP + #FP), \textbf{power} defined as #TP/$k$, \textbf{MSE} $= MSE(\hat\beta)$ / $||\beta||^2$ and \textbf{MSP} $= MSE(X\hat\beta)$ / $||X\beta||^2$. The results are based on 200 simulation runs.

The first simulation compares the following methods:

\begin{itemize}

\item \textbf{mBIC2} with stepwise searching procedure provided in \textit{bigstep} library (cyan curve),

\item \textit{L0Learn.fit} with \textbf{penalty L0L1} with the most accurate support size available (purple curve),

\item \textit{L0Learn.fit} with \textbf{penalty L0L2} with the most accurate support size available (green curve),

\item \textit{L0Learn.fit} with \textbf{CV penalty L0L2} - cross validation is performed firstly on $\gamma$ and then on $\lambda$ (red curve).

\end{itemize}

By selecting the most accurate support size available we mean picking from object generated by L0Learn.fit the model, which support size is closest to $k$. Such rule is very theoretical and requires a very strong assumption, but it is easy to come up with to start off and get first results. The idea of this experiment was to try to use functions from \textit{L0Learn} library, so the results should not be examined too carefully, notably the fourth experiment is precisely the example usage of \textit{L0Learn.cvfit} from \textit{L0Learn} library vignette. 

```{r plot1, fig.align='center', echo=FALSE, fig.height=6.8}


par(mfrow = c(2,2))
par(mar = c(3,3,3,3))
FDR_ncorr_weak = rbind(means_ncorr_weakx[1,], means_ncorr_weak_l1[1,], means_ncorr_weak_l2[1,], means_ncorr_weak_cvl2[1,])
matplot(c(10, 20, 40, 60, 80, 100), t(FDR_ncorr_weak), xlab = "", ylab = "",ylim = c(0,1), main = "FDR_NoCorr_Weak", pch = 19, type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('topleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

FDR_corr_weak = rbind(means_corr_weakx[1,], means_corr_weak_l1[1,], means_corr_weak_l2[1,], means_corr_weak_cvl2[1,])
matplot(c(10, 20, 40, 60, 80, 100), t(FDR_corr_weak), xlab = "", ylab = "", main = "FDR_Corr_Weak", pch = 19,ylim = c(0,1), type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('topleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

FDR_ncorr_strong = rbind(means_ncorr_strongx[1,], means_ncorr_strong_l1[1,], means_ncorr_strong_l2[1,], means_ncorr_strong_cvl2[1,])
matplot(c(10, 20, 40, 60, 80, 100), t(FDR_ncorr_strong), xlab = "", ylab = "", main = "FDR_NoCorr_Strong",ylim = c(0,1), pch = 19, type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('topleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

FDR_corr_strong = rbind(means_corr_strongx[1,], means_corr_strong_l1[1,], means_corr_strong_l2[1,], means_corr_strong_cvl2[1,])
matplot(c(10, 20, 40, 60, 80, 100), t(FDR_corr_strong), xlab = "", ylab = "", main = "FDR_Corr_Strong", ylim = c(0,1), pch = 19, type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('topleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

power_ncorr_weak = rbind(means_ncorr_weakx[2,], means_ncorr_weak_l1[2,], means_ncorr_weak_l2[2,], means_ncorr_weak_cvl2[2,])
matplot(c(10, 20, 40, 60, 80, 100), t(power_ncorr_weak), xlab = "", ylab = "", main = "Power_NoCorr_Weak", ylim = c(0,1), pch = 19, type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('bottomleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

power_corr_weak = rbind(means_corr_weakx[2,], means_corr_weak_l1[2,], means_corr_weak_l2[2,], means_corr_weak_cvl2[2,])
matplot(c(10, 20, 40, 60, 80, 100), t(power_corr_weak), xlab = "", ylab = "", main = "Power_Corr_Weak", ylim = c(0,1), pch = 19, type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('bottomleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

power_ncorr_strong = rbind(means_ncorr_strongx[2,], means_ncorr_strong_l1[2,], means_ncorr_strong_l2[2,], means_ncorr_strong_cvl2[2,])
matplot(c(10, 20, 40, 60, 80, 100), t(power_ncorr_strong), xlab = "", ylab = "", main = "Power_NoCorr_Strong", ylim = c(0,1), pch = 19, type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('bottomleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

power_corr_strong = rbind(means_corr_strongx[2,], means_corr_strong_l1[2,], means_corr_strong_l2[2,], means_corr_strong_cvl2[2,])
matplot(c(10, 20, 40, 60, 80, 100), t(power_corr_strong), xlab = "", ylab = "", main = "Power_Corr_Strong", ylim = c(0,1), pch = 19, type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('bottomleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

MSE_ncorr_weak = rbind(means_ncorr_weakx[3,], means_ncorr_weak_l1[3,], means_ncorr_weak_l2[3,], means_ncorr_weak_cvl2[3,])
matplot(c(10, 20, 40, 60, 80, 100), t(MSE_ncorr_weak), xlab = "", ylab = "", main = "MSE_NoCorr_Weak", pch = 19, type = 'o',ylim = c(0,1), col = c('cyan3','deeppink3','green3','red'))
legend('topleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

MSE_corr_weak = rbind(means_corr_weakx[3,], means_corr_weak_l1[3,], means_corr_weak_l2[3,], means_corr_weak_cvl2[3,])
matplot(c(10, 20, 40, 60, 80, 100), t(MSE_corr_weak), xlab = "", ylab = "", main = "MSE_Corr_Weak", pch = 19,ylim = c(0,1), type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('bottomleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

MSE_ncorr_strong = rbind(means_ncorr_strongx[3,], means_ncorr_strong_l1[3,], means_ncorr_strong_l2[3,], means_ncorr_strong_cvl2[3,])
matplot(c(10, 20, 40, 60, 80, 100), t(MSE_ncorr_strong), xlab = "", ylab = "", main = "MSE_NoCorr_Strong", pch = 19,ylim = c(0,1), type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('top', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

MSE_corr_strong = rbind(means_corr_strongx[3,], means_corr_strong_l1[3,], means_corr_strong_l2[3,], means_corr_strong_cvl2[3,])
matplot(c(10, 20, 40, 60, 80, 100), t(MSE_corr_strong), xlab = "", ylab = "", main = "MSE_Corr_Strong", pch = 19,ylim = c(0,1), type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('topleft', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

MSP_ncorr_weak = rbind(means_ncorr_weakx[4,], means_ncorr_weak_l1[4,], means_ncorr_weak_l2[4,], means_ncorr_weak_cvl2[4,])
matplot(c(10, 20, 40, 60, 80, 100), t(MSP_ncorr_weak), xlab = "", ylab = "",ylim = c(0,1), main = "MSP_NoCorr_Weak", pch = 19, type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('top', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

MSP_corr_weak = rbind(means_corr_weakx[4,], means_corr_weak_l1[4,], means_corr_weak_l2[4,], means_corr_weak_cvl2[4,])
matplot(c(10, 20, 40, 60, 80, 100), t(MSP_corr_weak), xlab = "", ylab = "",ylim = c(0,1), main = "MSP_Corr_Weak", pch = 19,type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('topright', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

MSP_ncorr_strong = rbind(means_ncorr_strongx[4,], means_ncorr_strong_l1[4,], means_ncorr_strong_l2[4,], means_ncorr_strong_cvl2[4,])
matplot(c(10, 20, 40, 60, 80, 100), t(MSP_ncorr_strong), xlab = "", ylab = "",ylim = c(0,1), main = "MSP_NoCorr_Strong", pch = 19, type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('top', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))

MSP_corr_strong = rbind(means_corr_strongx[4,], means_corr_strong_l1[4,], means_corr_strong_l2[4,], means_corr_strong_cvl2[4,])
matplot(c(10, 20, 40, 60, 80, 100), t(MSP_corr_strong), xlab = "", ylab = "",ylim = c(0,1), main = "MSP_Corr_Strong", pch = 19, type = 'o', col = c('cyan3','deeppink3','green3','red'))
legend('topright', c('MBIC2','pen. L0L1','pen. L0L2','CV pen. L0L2'), pch = 19, col = c('cyan3','deeppink3','green3','red'))


```

The second simulation compares the following methods:

\begin{itemize}

\item \textbf{mBIC2} with stepwise searching procedure provided in \textit{bigstep} library - same as in the first simulation (cyan curve)

\item \textit{L0Learn.fit} with \textbf{penalty L0} with the most accurate support size available (green curve),

\item \textit{L0Learn.fit} \textbf{penalty L0} with the support size most accurate to mBIC2 (purple curve).

\end{itemize}

Here we focus only on the optimalisation with only L0 penalty. The second experiment is similar to two from in the first simulation. The third experiment is an attempt to apply a more serious method of selecting model from object returned by \textit{L0Learn.fit}. On the contrary to the first approach, we choose the model, which support size is closest to the one chosen by mBIC2.

```{r plot2, fig.align='center', echo=FALSE, fig.height=6.8}


legend2 = c('MBIC2','k(MBIC2) -> L0', 'perfect L0')
cl = c('cyan3','deeppink3','green3')


  par(mfrow = c(2,2))
  par(mar = c(3,3,3,3))
  FDR_ncorr_weak = rbind(means_ncorr_weakx[1,], means_ncorr_weak_mbic2l0[1,], means_ncorr_weak_l0[1,])
  matplot(c(10, 20, 40, 60, 80, 100), t(FDR_ncorr_weak), xlab = "", ylab = "",ylim = c(0,1), main = "FDR_NoCorr_Weak", pch = 19, type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  FDR_corr_weak = rbind(means_corr_weakx[1,], means_corr_weak_mbic2l0[1,], means_corr_weak_l0[1,])
  matplot(c(10, 20, 40, 60, 80, 100), t(FDR_corr_weak), xlab = "", ylab = "", main = "FDR_Corr_Weak", pch = 19,ylim = c(0,1), type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  FDR_ncorr_strong = rbind(means_ncorr_strongx[1,], means_ncorr_strong_mbic2l0[1,], means_ncorr_strong_l0[1,])
  matplot(c(10, 20, 40, 60, 80, 100), t(FDR_ncorr_strong), xlab = "", ylab = "", main = "FDR_NoCorr_Strong",ylim = c(0,1), pch = 19, type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  FDR_corr_strong = rbind(means_corr_strongx[1,], means_corr_strong_mbic2l0[1,], means_corr_strong_l0[1,])
  matplot(c(10, 20, 40, 60, 80, 100), t(FDR_corr_strong), xlab = "", ylab = "", main = "FDR_Corr_Strong", ylim = c(0,1), pch = 19, type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  power_ncorr_weak = rbind(means_ncorr_weakx[2,], means_ncorr_weak_mbic2l0[2,], means_ncorr_weak_l0[2,])
  matplot(c(10, 20, 40, 60, 80, 100), t(power_ncorr_weak), xlab = "", ylab = "", main = "Power_NoCorr_Weak", ylim = c(0,1), pch = 19, type = 'o', col = cl)
  legend('bottomleft', legend2, pch = 19, col = cl)
  
  power_corr_weak = rbind(means_corr_weakx[2,], means_corr_weak_mbic2l0[2,], means_corr_weak_l0[2,])
  matplot(c(10, 20, 40, 60, 80, 100), t(power_corr_weak), xlab = "", ylab = "", main = "Power_Corr_Weak", ylim = c(0,1), pch = 19, type = 'o', col = cl)
  legend('bottomleft', legend2, pch = 19, col = cl)
  
  power_ncorr_strong = rbind(means_ncorr_strongx[2,], means_ncorr_strong_mbic2l0[2,], means_ncorr_strong_l0[2,])
  matplot(c(10, 20, 40, 60, 80, 100), t(power_ncorr_strong), xlab = "", ylab = "", main = "Power_NoCorr_Strong", ylim = c(0,1), pch = 19, type = 'o', col = cl)
  legend('bottomleft', legend2, pch = 19, col = cl)
  
  power_corr_strong = rbind(means_corr_strongx[2,], means_corr_strong_mbic2l0[2,], means_corr_strong_l0[2,])
  matplot(c(10, 20, 40, 60, 80, 100), t(power_corr_strong), xlab = "", ylab = "", main = "Power_Corr_Strong", ylim = c(0,1), pch = 19, type = 'o', col = cl)
  legend('bottomleft', legend2, pch = 19, col = cl)
  
  MSE_ncorr_weak = rbind(means_ncorr_weakx[3,], means_ncorr_weak_mbic2l0[3,], means_ncorr_weak_l0[3,])
  matplot(c(10, 20, 40, 60, 80, 100), t(MSE_ncorr_weak), xlab = "", ylab = "", main = "MSE_NoCorr_Weak", pch = 19, type = 'o',ylim = c(0,5), col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  MSE_corr_weak = rbind(means_corr_weakx[3,], means_corr_weak_mbic2l0[3,], means_corr_weak_l0[3,])
  matplot(c(10, 20, 40, 60, 80, 100), t(MSE_corr_weak), xlab = "", ylab = "", main = "MSE_Corr_Weak", pch = 19,ylim = c(0,5), type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  MSE_ncorr_strong = rbind(means_ncorr_strongx[3,], means_ncorr_strong_mbic2l0[3,], means_ncorr_strong_l0[3,])
  matplot(c(10, 20, 40, 60, 80, 100), t(MSE_ncorr_strong), xlab = "", ylab = "", main = "MSE_NoCorr_Strong", pch = 19,ylim = c(0,5), type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  MSE_corr_strong = rbind(means_corr_strongx[3,], means_corr_strong_mbic2l0[3,], means_corr_strong_l0[3,])
  matplot(c(10, 20, 40, 60, 80, 100), t(MSE_corr_strong), xlab = "", ylab = "", main = "MSE_Corr_Strong", pch = 19,ylim = c(0,5), type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  MSP_ncorr_weak = rbind(means_ncorr_weakx[4,], means_ncorr_weak_mbic2l0[4,], means_ncorr_weak_l0[4,])
  matplot(c(10, 20, 40, 60, 80, 100), t(MSP_ncorr_weak), xlab = "", ylab = "",ylim = c(0,1), main = "MSP_NoCorr_Weak", pch = 19, type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  MSP_corr_weak = rbind(means_corr_weakx[4,], means_corr_weak_mbic2l0[4,], means_corr_weak_l0[4,])
  matplot(c(10, 20, 40, 60, 80, 100), t(MSP_corr_weak), xlab = "", ylab = "",ylim = c(0,1), main = "MSP_Corr_Weak", pch = 19,type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  MSP_ncorr_strong = rbind(means_ncorr_strongx[4,], means_ncorr_strong_mbic2l0[4,], means_ncorr_strong_l0[4,])
  matplot(c(10, 20, 40, 60, 80, 100), t(MSP_ncorr_strong), xlab = "", ylab = "",ylim = c(0,1), main = "MSP_NoCorr_Strong", pch = 19, type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)
  
  MSP_corr_strong = rbind(means_corr_strongx[4,], means_corr_strong_mbic2l0[4,], means_corr_strong_l0[4,])
  matplot(c(10, 20, 40, 60, 80, 100), t(MSP_corr_strong), xlab = "", ylab = "",ylim = c(0,1), main = "MSP_Corr_Strong", pch = 19, type = 'o', col = cl)
  legend('topleft', legend2, pch = 19, col = cl)


```

Here we focus only on the optimalisation with only L0 penalty. The second experiment is similar to two from in the first simulation. The third experiment is an attempt to apply a more serious method of selecting a model from object returned by \textit{L0Learn.fit}. In the contrary to the first approach, we choose the model, which support size is closest to the support size of the one chosen by mBIC2.

We can notice, that if we can correctly estimate the number of nonzero coefficients, we obtain very good results. The output of \textit{L0Learn.fit} very frequently contains a good model, so discovering a well-performing method of selection should be further investigated. Thus we can associate the green curve with the target results of further research.

We see, that when the regressors are independent, the results of this method and mBIC2 are almost equal, with some small differences in FDR and power in the favor of \textit{L0Learn.fit}. In case of correlation mBIC2 produces better results, with the differences being larger in strong signal case. 
